\section{Background information}

\begin{frame}{What is regression?}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Regreesion fits a line, plane or hyperplane through a set of points in $n$ dimensional space.
                \item This fitting is done by treating the "response" variable as a function of other "predictor" variables.
                \item The fit is improved by minimizing the lines overall distance from the points, otherwise known as the residuals.
                \item This is known as minimizing the "residual sum of squares", or, RSS.
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[scale=0.45]{img/residual.png}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Penalized Regression}
    \begin{itemize}
        \item Having more predictors can often improve model performance, but at the cost of increased complexity.
        \item There are many consequences to overly complex models, so penalized regression models attempt to help with this.
        \item LASSO regression is surprise, a type of penalized regression.
    \end{itemize}
\end{frame}

\begin{frame}{Penalized Regression Structure}
    Penalized regression is made up of two chunks. The RSS part shared with linear regression, and a new penalty chunk.

    \begin{align*}
        \text{Linear Regression: } \quad &\min \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
        \text{LASSO Regression: } \quad
        &\min \underbrace{ \sum_{i=1}^n (y_i - \hat{y}_i)^2}_{\text{RSS}}
        +
        \underbrace{\lambda \sum_{j=1}^p |b_j|}_{\text{Penalty}} \\
        \hat{y}_i &= \sum_{j=1}^p b_j x_{ij}
    \end{align*}
\end{frame}

\begin{frame}{LASSO Regression Features}
    \begin{itemize}
        \item The penalty is very useful because it can drag coefficients all the way down to zero.
        \item What this means is LASSO regression can completely remove unimportant variables from a model!
        \item This makes it a convenient variable selection tool.
    \end{itemize}
\end{frame}

\begin{frame}{LASSO as a Quadratic Program (QP) - Setup}
    \begin{itemize}
        \item Converting this into the form of an LP is pretty straightforward, but some modifications are needed.
        \item The RSS portion becomes the objective function, but it's squared so this is actually a quadratic program (QP).
        \item The penalty will become the constraint, but the absolute value is inappropriate for an LP (or QP). 
        \item To fix that, we let $b_j = b_j^+ - b_j^-$ where $b_j^+ , b_j^- \ge 0$. The absolute value then is just $|b_j| = b_j^+ + b_j^-$.
    \end{itemize}
\end{frame}

\begin{frame}{LASSO as a Quadratic Program (QP) - Execution}
    This gives us the following program.
    
    \begin{align*}
        \min &\sum_i (y_i - \hat{y}_i)^2 \\
        \text{subject to: } &\sum_{j=1}^p (b_j^+ + b_j^-) \le t \\
        \hat{y}_i &= \sum_j b_j^+ x_{ij} - \sum_j b_j^- x_{ij} \\
        b_j^+, b_j^- &\ge 0
    \end{align*}
\end{frame}