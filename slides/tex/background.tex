\section{Background information}

\begin{frame}{What is regression?}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Regression is a statistical technique that allows us to see the relationship between a \textbf{dependent variable} and a set of \textbf{independent variables}.
                \item It is commonly known as a line fit through a set of data points! 
                \item The most basic form of it is $y = \beta_0 + \beta_1 x + \epsilon$ Where the $\beta$ coefficients are estimated from the data and $\epsilon$ represents random error unexplained by the model.
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[scale=0.45]{img/simple-lin-reg.jpg}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{How is the regression line chosen?}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item But how is that line created? How are those $\beta$ coefficients estimated? 
                \item In most situations, no line will perfectly fit the data so we want it to be as close as possible.
                \item The distance from the line to a given data point is called a \textbf{residual}, we want these to be small.
                \item So, linear regression seeks to minimize the \textbf{residual sum of squares}, or \textbf{RSS}.
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[scale=0.45]{img/residual.png}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Why isn't Linear Regression an LP?}
    Our goal to minimize the RSS can be represented as:

    \vspace{-6mm}
    \begin{align*}
        &\min \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
        &\hat{y}_i = \sum_{j=1}^p \beta_j x_{ij}
    \end{align*}

    Where $y_i$ is the actual response value, and $\hat{y}_i$ is that value estimated by our model. 

    That looks like an objective function, but it's missing something. The $\beta$ values are completely unconstrained! They could be anything. 
    
    So, simple linear regression can't be formulated as an LP. That's where penalized regression comes in.
\end{frame}

% \begin{frame}{What is regression?}
%     \begin{columns}
%         \begin{column}{0.6\textwidth}
%             \begin{itemize}
%                 \item Regreesion fits a line, plane or hyperplane through a set of points in $n$ dimensional space.
%                 \item This fitting is done by treating the "response" variable as a function of other "predictor" variables.
%                 \item The fit is improved by minimizing the lines overall distance from the points, otherwise known as the residuals.
%                 \item This is known as minimizing the "residual sum of squares", or, RSS.
%             \end{itemize}
%         \end{column}
%         \begin{column}{0.4\textwidth}
%             \includegraphics[scale=0.45]{img/residual.png}
%         \end{column}
%     \end{columns}
% \end{frame}

\begin{frame}{Penalized Regression}
    \begin{itemize}
        \item In statistical modeling, it can often be advantageous to have as simple a model as possible. Thus, there are tools that penalize overly complex models.
        \item These tools come in different forms. There are metrics that take the number of dependent variables into account, as well as models with built in penalties that affect the coefficients.
        \item We're interested in an example of the latter: \textbf{LASSO regression}, a type of \textbf{penalized regression}.
    \end{itemize}
\end{frame}

\begin{frame}{Penalized Regression}
    LASSO regression is made up of two chunks. The RSS part shared with linear regression, and a new penalty chunk. 

    \vspace{-6mm}
    \begin{align*}
        \text{Linear Regression: } \quad &\min \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
        \text{LASSO Regression: } \quad
        &\min \underbrace{ \sum_{i=1}^n (y_i - \hat{y}_i)^2}_{\text{RSS}}
        +
        \underbrace{\lambda \sum_{j=1}^p |b_j|}_{\text{Penalty}}
        % \hat{y}_i &= \sum_{j=1}^p b_j x_{ij}
    \end{align*}

    That penalty is the key here.
\end{frame}

\begin{frame}{LASSO Regression Penalty}
    The penalty has some useful features and consequences.
    \begin{itemize}
        \item The penalty can drag coefficients all the way down to zero.
        \item Because of this, LASSO regression can completely remove unimportant variables from a model!
        \item This makes it a convenient variable selection tool.
        \item When re-written, the penalty also gives us the \textbf{constraint} we need for an LP.
    \end{itemize}
\end{frame}

\begin{frame}{LASSO as a Quadratic Program (QP) - Setup}
    \begin{itemize}
        \item Converting this into the form of an LP is pretty straightforward, but some modifications are needed.
        \item The RSS portion becomes the objective function, but it's squared so this is actually a quadratic program (QP).
        \item The penalty will become the constraint, but the absolute value is inappropriate for an LP (or QP). 
        \item To fix that, we let $b_j = b_j^+ - b_j^-$ where $b_j^+ , b_j^- \ge 0$. The absolute value then is just $|b_j| = b_j^+ + b_j^-$.
    \end{itemize}
\end{frame}

\begin{frame}{LASSO as a Quadratic Program (QP) - Execution}
    This gives us the following program.
    
    \vspace{-6mm}
    \begin{align*}
        \min &\sum_i (y_i - \hat{y}_i)^2 \\
        \text{subject to: } &\sum_{j=1}^p (b_j^+ + b_j^-) \le t \\
        \hat{y}_i &= \sum_j b_j^+ x_{ij} - \sum_j b_j^- x_{ij} \\
        b_j^+, b_j^- &\ge 0
    \end{align*}
\end{frame}